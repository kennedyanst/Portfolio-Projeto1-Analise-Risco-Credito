{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizando algoritmo classificador de Rede Neural Artificial do scikit-learn: Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando a biblioteca para a classificação por Redes Neurais Artificiais (Multi-Layer Perceptron) e a biblioteca que irá carregar os dados de treinamento e teste\n",
    "import pickle\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"base.pkl\", \"rb\") as df:\n",
    "    X_base_treinamento, y_base_treinamento, X_base_teste, y_base_teste = pickle.load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 3), (1500,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando o shape dos atributos\n",
    "X_base_treinamento.shape, y_base_treinamento.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 3), (500,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_base_teste.shape, y_base_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.59379818\n",
      "Iteration 2, loss = 0.45248840\n",
      "Iteration 3, loss = 0.35689148\n",
      "Iteration 4, loss = 0.28489732\n",
      "Iteration 5, loss = 0.22804996\n",
      "Iteration 6, loss = 0.18790165\n",
      "Iteration 7, loss = 0.15638546\n",
      "Iteration 8, loss = 0.13345623\n",
      "Iteration 9, loss = 0.11720282\n",
      "Iteration 10, loss = 0.10521745\n",
      "Iteration 11, loss = 0.09580952\n",
      "Iteration 12, loss = 0.08860649\n",
      "Iteration 13, loss = 0.08266478\n",
      "Iteration 14, loss = 0.07697594\n",
      "Iteration 15, loss = 0.07244313\n",
      "Iteration 16, loss = 0.06752359\n",
      "Iteration 17, loss = 0.06454389\n",
      "Iteration 18, loss = 0.05991747\n",
      "Iteration 19, loss = 0.05770333\n",
      "Iteration 20, loss = 0.05399834\n",
      "Iteration 21, loss = 0.05146260\n",
      "Iteration 22, loss = 0.04878893\n",
      "Iteration 23, loss = 0.04662753\n",
      "Iteration 24, loss = 0.04448602\n",
      "Iteration 25, loss = 0.04262786\n",
      "Iteration 26, loss = 0.04079218\n",
      "Iteration 27, loss = 0.03942295\n",
      "Iteration 28, loss = 0.03762856\n",
      "Iteration 29, loss = 0.03602582\n",
      "Iteration 30, loss = 0.03501978\n",
      "Iteration 31, loss = 0.03360972\n",
      "Iteration 32, loss = 0.03247011\n",
      "Iteration 33, loss = 0.03152660\n",
      "Iteration 34, loss = 0.03024115\n",
      "Iteration 35, loss = 0.02975119\n",
      "Iteration 36, loss = 0.02922192\n",
      "Iteration 37, loss = 0.02839893\n",
      "Iteration 38, loss = 0.02701409\n",
      "Iteration 39, loss = 0.02625290\n",
      "Iteration 40, loss = 0.02585567\n",
      "Iteration 41, loss = 0.02502675\n",
      "Iteration 42, loss = 0.02424168\n",
      "Iteration 43, loss = 0.02377096\n",
      "Iteration 44, loss = 0.02326646\n",
      "Iteration 45, loss = 0.02340646\n",
      "Iteration 46, loss = 0.02228167\n",
      "Iteration 47, loss = 0.02213932\n",
      "Iteration 48, loss = 0.02110785\n",
      "Iteration 49, loss = 0.02066936\n",
      "Iteration 50, loss = 0.02010208\n",
      "Iteration 51, loss = 0.02013795\n",
      "Iteration 52, loss = 0.01966712\n",
      "Iteration 53, loss = 0.01940721\n",
      "Iteration 54, loss = 0.01866516\n",
      "Iteration 55, loss = 0.01891901\n",
      "Iteration 56, loss = 0.01841286\n",
      "Iteration 57, loss = 0.01769120\n",
      "Iteration 58, loss = 0.01767967\n",
      "Iteration 59, loss = 0.01686732\n",
      "Iteration 60, loss = 0.01643373\n",
      "Iteration 61, loss = 0.01650475\n",
      "Iteration 62, loss = 0.01626560\n",
      "Iteration 63, loss = 0.01580968\n",
      "Iteration 64, loss = 0.01539722\n",
      "Iteration 65, loss = 0.01568117\n",
      "Iteration 66, loss = 0.01508798\n",
      "Iteration 67, loss = 0.01463160\n",
      "Iteration 68, loss = 0.01478030\n",
      "Iteration 69, loss = 0.01450874\n",
      "Iteration 70, loss = 0.01363987\n",
      "Iteration 71, loss = 0.01408762\n",
      "Iteration 72, loss = 0.01359970\n",
      "Iteration 73, loss = 0.01459074\n",
      "Iteration 74, loss = 0.01320781\n",
      "Iteration 75, loss = 0.01474894\n",
      "Iteration 76, loss = 0.01296336\n",
      "Iteration 77, loss = 0.01257319\n",
      "Iteration 78, loss = 0.01221912\n",
      "Iteration 79, loss = 0.01204499\n",
      "Iteration 80, loss = 0.01207175\n",
      "Iteration 81, loss = 0.01192567\n",
      "Iteration 82, loss = 0.01162595\n",
      "Iteration 83, loss = 0.01197562\n",
      "Iteration 84, loss = 0.01134279\n",
      "Iteration 85, loss = 0.01100630\n",
      "Iteration 86, loss = 0.01108115\n",
      "Iteration 87, loss = 0.01090969\n",
      "Iteration 88, loss = 0.01091703\n",
      "Iteration 89, loss = 0.01059247\n",
      "Iteration 90, loss = 0.01043291\n",
      "Iteration 91, loss = 0.01033082\n",
      "Iteration 92, loss = 0.01052737\n",
      "Iteration 93, loss = 0.01022392\n",
      "Iteration 94, loss = 0.00984836\n",
      "Iteration 95, loss = 0.00957516\n",
      "Iteration 96, loss = 0.00952470\n",
      "Iteration 97, loss = 0.00962824\n",
      "Iteration 98, loss = 0.00932579\n",
      "Iteration 99, loss = 0.00923242\n",
      "Iteration 100, loss = 0.00885590\n",
      "Iteration 101, loss = 0.00895799\n",
      "Iteration 102, loss = 0.00879339\n",
      "Iteration 103, loss = 0.00899325\n",
      "Iteration 104, loss = 0.00862959\n",
      "Iteration 105, loss = 0.00870379\n",
      "Iteration 106, loss = 0.00871184\n",
      "Iteration 107, loss = 0.00822443\n",
      "Iteration 108, loss = 0.00904871\n",
      "Iteration 109, loss = 0.00887681\n",
      "Iteration 110, loss = 0.00856834\n",
      "Iteration 111, loss = 0.00816007\n",
      "Iteration 112, loss = 0.00790924\n",
      "Iteration 113, loss = 0.00803397\n",
      "Iteration 114, loss = 0.00790155\n",
      "Iteration 115, loss = 0.00829666\n",
      "Iteration 116, loss = 0.00759912\n",
      "Iteration 117, loss = 0.00750647\n",
      "Iteration 118, loss = 0.00732075\n",
      "Iteration 119, loss = 0.00740560\n",
      "Iteration 120, loss = 0.00757116\n",
      "Iteration 121, loss = 0.00750571\n",
      "Iteration 122, loss = 0.00738013\n",
      "Iteration 123, loss = 0.00763075\n",
      "Iteration 124, loss = 0.00682764\n",
      "Iteration 125, loss = 0.00728659\n",
      "Iteration 126, loss = 0.00688036\n",
      "Iteration 127, loss = 0.00673409\n",
      "Iteration 128, loss = 0.00669336\n",
      "Iteration 129, loss = 0.00668775\n",
      "Iteration 130, loss = 0.00629447\n",
      "Iteration 131, loss = 0.00646079\n",
      "Iteration 132, loss = 0.00645172\n",
      "Iteration 133, loss = 0.00642546\n",
      "Iteration 134, loss = 0.00634786\n",
      "Iteration 135, loss = 0.00651941\n",
      "Iteration 136, loss = 0.00602427\n",
      "Iteration 137, loss = 0.00624255\n",
      "Iteration 138, loss = 0.00589231\n",
      "Iteration 139, loss = 0.00595251\n",
      "Iteration 140, loss = 0.00580368\n",
      "Iteration 141, loss = 0.00582657\n",
      "Iteration 142, loss = 0.00585750\n",
      "Iteration 143, loss = 0.00553385\n",
      "Iteration 144, loss = 0.00554748\n",
      "Iteration 145, loss = 0.00540197\n",
      "Iteration 146, loss = 0.00530981\n",
      "Iteration 147, loss = 0.00531471\n",
      "Iteration 148, loss = 0.00550386\n",
      "Iteration 149, loss = 0.00524913\n",
      "Iteration 150, loss = 0.00567458\n",
      "Iteration 151, loss = 0.00520674\n",
      "Iteration 152, loss = 0.00539087\n",
      "Iteration 153, loss = 0.00593176\n",
      "Iteration 154, loss = 0.00698083\n",
      "Iteration 155, loss = 0.00743570\n",
      "Iteration 156, loss = 0.00565290\n",
      "Iteration 157, loss = 0.00713549\n",
      "Iteration 158, loss = 0.00526409\n",
      "Iteration 159, loss = 0.00484752\n",
      "Iteration 160, loss = 0.00497146\n",
      "Iteration 161, loss = 0.00484513\n",
      "Iteration 162, loss = 0.00471536\n",
      "Iteration 163, loss = 0.00465684\n",
      "Iteration 164, loss = 0.00466600\n",
      "Iteration 165, loss = 0.00441837\n",
      "Iteration 166, loss = 0.00463926\n",
      "Iteration 167, loss = 0.00454998\n",
      "Iteration 168, loss = 0.00441429\n",
      "Iteration 169, loss = 0.00429206\n",
      "Iteration 170, loss = 0.00444547\n",
      "Iteration 171, loss = 0.00432058\n",
      "Iteration 172, loss = 0.00429039\n",
      "Iteration 173, loss = 0.00416298\n",
      "Iteration 174, loss = 0.00421351\n",
      "Iteration 175, loss = 0.00409839\n",
      "Iteration 176, loss = 0.00401948\n",
      "Iteration 177, loss = 0.00413244\n",
      "Iteration 178, loss = 0.00432904\n",
      "Iteration 179, loss = 0.00404985\n",
      "Iteration 180, loss = 0.00395773\n",
      "Iteration 181, loss = 0.00403778\n",
      "Iteration 182, loss = 0.00398232\n",
      "Iteration 183, loss = 0.00400361\n",
      "Iteration 184, loss = 0.00384687\n",
      "Iteration 185, loss = 0.00383049\n",
      "Iteration 186, loss = 0.00388312\n",
      "Iteration 187, loss = 0.00353069\n",
      "Iteration 188, loss = 0.00380100\n",
      "Iteration 189, loss = 0.00360078\n",
      "Iteration 190, loss = 0.00370999\n",
      "Iteration 191, loss = 0.00371746\n",
      "Iteration 192, loss = 0.00350110\n",
      "Iteration 193, loss = 0.00359768\n",
      "Iteration 194, loss = 0.00356947\n",
      "Iteration 195, loss = 0.00360700\n",
      "Iteration 196, loss = 0.00395222\n",
      "Iteration 197, loss = 0.00349042\n",
      "Iteration 198, loss = 0.00366308\n",
      "Iteration 199, loss = 0.00330745\n",
      "Iteration 200, loss = 0.00345343\n",
      "Iteration 201, loss = 0.00326028\n",
      "Iteration 202, loss = 0.00335392\n",
      "Iteration 203, loss = 0.00334066\n",
      "Iteration 204, loss = 0.00325044\n",
      "Iteration 205, loss = 0.00345737\n",
      "Iteration 206, loss = 0.00331937\n",
      "Iteration 207, loss = 0.00353411\n",
      "Iteration 208, loss = 0.00322378\n",
      "Iteration 209, loss = 0.00347491\n",
      "Iteration 210, loss = 0.00314677\n",
      "Iteration 211, loss = 0.00324319\n",
      "Iteration 212, loss = 0.00311609\n",
      "Iteration 213, loss = 0.00388545\n",
      "Iteration 214, loss = 0.00380787\n",
      "Iteration 215, loss = 0.00411298\n",
      "Iteration 216, loss = 0.00341837\n",
      "Iteration 217, loss = 0.00318991\n",
      "Iteration 218, loss = 0.00292069\n",
      "Iteration 219, loss = 0.00328126\n",
      "Iteration 220, loss = 0.00344276\n",
      "Iteration 221, loss = 0.00279949\n",
      "Iteration 222, loss = 0.00296992\n",
      "Iteration 223, loss = 0.00275296\n",
      "Iteration 224, loss = 0.00274797\n",
      "Iteration 225, loss = 0.00290341\n",
      "Iteration 226, loss = 0.00300658\n",
      "Iteration 227, loss = 0.00286193\n",
      "Iteration 228, loss = 0.00274589\n",
      "Iteration 229, loss = 0.00267586\n",
      "Iteration 230, loss = 0.00283416\n",
      "Iteration 231, loss = 0.00265285\n",
      "Iteration 232, loss = 0.00256940\n",
      "Iteration 233, loss = 0.00271208\n",
      "Iteration 234, loss = 0.00288698\n",
      "Iteration 235, loss = 0.00317026\n",
      "Iteration 236, loss = 0.00265732\n",
      "Iteration 237, loss = 0.00255763\n",
      "Iteration 238, loss = 0.00263140\n",
      "Iteration 239, loss = 0.00242649\n",
      "Iteration 240, loss = 0.00271675\n",
      "Iteration 241, loss = 0.00272322\n",
      "Iteration 242, loss = 0.00250812\n",
      "Iteration 243, loss = 0.00247107\n",
      "Iteration 244, loss = 0.00282698\n",
      "Iteration 245, loss = 0.00298147\n",
      "Iteration 246, loss = 0.00273772\n",
      "Iteration 247, loss = 0.00236363\n",
      "Iteration 248, loss = 0.00234709\n",
      "Iteration 249, loss = 0.00241361\n",
      "Iteration 250, loss = 0.00237356\n",
      "Iteration 251, loss = 0.00244387\n",
      "Iteration 252, loss = 0.00242220\n",
      "Iteration 253, loss = 0.00242704\n",
      "Iteration 254, loss = 0.00226385\n",
      "Iteration 255, loss = 0.00236173\n",
      "Iteration 256, loss = 0.00226385\n",
      "Iteration 257, loss = 0.00229830\n",
      "Iteration 258, loss = 0.00253021\n",
      "Iteration 259, loss = 0.00221534\n",
      "Iteration 260, loss = 0.00213377\n",
      "Iteration 261, loss = 0.00211115\n",
      "Iteration 262, loss = 0.00219688\n",
      "Iteration 263, loss = 0.00202588\n",
      "Iteration 264, loss = 0.00207874\n",
      "Iteration 265, loss = 0.00218863\n",
      "Iteration 266, loss = 0.00212880\n",
      "Iteration 267, loss = 0.00210015\n",
      "Iteration 268, loss = 0.00218537\n",
      "Iteration 269, loss = 0.00203173\n",
      "Iteration 270, loss = 0.00197349\n",
      "Iteration 271, loss = 0.00200525\n",
      "Iteration 272, loss = 0.00197474\n",
      "Iteration 273, loss = 0.00197658\n",
      "Iteration 274, loss = 0.00185904\n",
      "Iteration 275, loss = 0.00185818\n",
      "Iteration 276, loss = 0.00190850\n",
      "Iteration 277, loss = 0.00186700\n",
      "Iteration 278, loss = 0.00191940\n",
      "Iteration 279, loss = 0.00193011\n",
      "Iteration 280, loss = 0.00186287\n",
      "Iteration 281, loss = 0.00195615\n",
      "Iteration 282, loss = 0.00257075\n",
      "Iteration 283, loss = 0.00209305\n",
      "Iteration 284, loss = 0.00250560\n",
      "Iteration 285, loss = 0.00234813\n",
      "Iteration 286, loss = 0.00203299\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1500, random_state=1,\n",
       "              tol=1e-09, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1500, random_state=1,\n",
       "              tol=1e-09, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1500, random_state=1,\n",
       "              tol=1e-09, verbose=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural = MLPClassifier(max_iter=1500, verbose=True, tol=0.000000001, solver= \"adam\", activation= \"relu\", hidden_layer_sizes=(100,100), random_state=1)\n",
    "rede_neural.fit(X_base_treinamento, y_base_treinamento) #Maximum interation = Epocas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sobre os parâmetros utilizados:\n",
    "* max_iter: Foi utilizado 1500 épocas para diminuir o número de erros nessa base, visto que o valor padrão de 200 épocas não se mostrou suficiente. Além do mais, foram testadas 800, 1000, 1200 épocas. Como a base do estudo não é grande, um número maior de épocas não foi necessário.  \n",
    "* verbose: Foi ajustado para \"True\" com o intuito de ser visualizado o valor do erro época por época. A mudança não faz diferença no resultado do algoritmo mas trouxe para o desenvolvedor um feedback sobre o andamento do código. \n",
    "* tol: Foi utilizado para fazer o algoritmo rodar por mais épocas até atingir o maior número de zeros na perda. \n",
    "* solver: A utilização do valor \"adam\" foi uma escolha padrão, visto que é o algoritmo mais utilizado na área de Deeplearning. O valor é uma melhoria da descida do gradiente e mesmo a base não sendo tão grande, resolvi usa-lo.\n",
    "* activation: A utilização do valor \"relu\" foi uma escolha padrão, visto que é a função de ativação mais utilizada na área de aprendizado de máquina\n",
    "* hidden_layer_sizes: A camada oculta foi uma escolha apriori, as configurações serão revistas futuramente com a função GridSearchCV() do sklearn\n",
    "* random_state: Para manter valor fixo dos testes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsao = rede_neural.predict(X_base_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A precisão do algoritmo foi de 99.6%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print(f\"A precisão do algoritmo foi de {accuracy_score(y_base_teste, previsao) * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.996"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizando onde está a maior precisão do algoritmo, se no pagante ou não-pagante. Neste caso, a precisão do algoritmo é quase 100%. O que se mostra o ideal, visto que o objetivo do algoritmo seria evitar a perda de dinheiro por emprestimo de um banco. Errou apenas uma classificação dos não-pagantes.\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "cm = ConfusionMatrix(rede_neural)\n",
    "cm.fit(X_base_treinamento, y_base_treinamento)\n",
    "cm.score(X_base_teste, y_base_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       436\n",
      "           1       0.98      0.98      0.98        64\n",
      "\n",
      "    accuracy                           1.00       500\n",
      "   macro avg       0.99      0.99      0.99       500\n",
      "weighted avg       1.00      1.00      1.00       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_base_teste, previsao))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O algoritmo consegue identificar corretamente 100% dos dados da classe 0 (Pagantes) com uma precisão de 100% e 98% dos dados da classe 1 (Não-Pagantes) com uma precisão de 98%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "366c4c5561b3b7192db4ea44ad2361e84217bc9c8517193206197b435f3a3215"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
